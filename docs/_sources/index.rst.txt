.. WFA documentation master file, created by
   sphinx-quickstart on Thu Feb  3 11:01:42 2022.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

.. toctree::
   :maxdepth: 2
   :caption: Contents:

Welcome to WFA's Documentation!
==================================
This page contains the documentation for the Wafer scale engine Field equation Application programming interfas or WFA.

This package is a product of the `National Energy Technology Laboratory <http://www.netl.doe.gov>`_ (NETL)
in collaboration with `Cerebras Systems Inc <https://cerebras.net/>`_.

Field equations have two properties:

1. They have a value or set of values at every point in space time

2. Changes to the values are only affected by the immediate surroundings (the Principle of Locality in physics)

This gives rise to several sets of common differential equations that describe
almost every physical phenomenon in nature at the finest space-time scales.
The only known exception to this fundamental principle appears to be quantum
entanglement which seems to have the property of instantaneous information
transfer.  Every other phenomenon propagates through space-time with a transfer
rate lower than or equal to the speed of light.  Field equations are even fundamental
to our understanding of the quantum realm at Plank scales.  Field equations
can be used to describe systems of objects which exhibit discrete behavior at 
intermediate scales. Due to the law of averages, ensembles of discrete objects can take
on the character of a field equation at scales that are many times larger than the 
discrete objects (although even "discrete" objects are described as
fields on the finest scales).  This allows sets of field equations to adequately
and accurately describe the behavior of fluids, star/galaxy clusters, and granular
solids under limited circumstances. If a physical system can be written with ODE/PDE 
expressions at the scales of interest, it is possible to approximate them on a computer 
through discretization.  

It should come as no surprise that an Application Specific Integrated Circuit (ASIC) can be
many orders of magnitude faster and more efficient than general purpose computing hardware.
To the best of our knowledge, no one has endeavored to create an ASIC for field equation modeling. 
However, recent developments in Artificial Intelligence (AI) ASICS are producing hardware that happens
to also be very good at field equation modeling, as is the case with the Cerebras Wafer Scale
Engine.  The architecture allows definition of discretized values, distributed evenly across the
processor array and mirrors the physical Principle of Locality in that it allows for the nearest 
neighbors to affect a local change in a minimum of clock cycles (on average 1-2 cycles).  
While the WSE architecture was not developed as an ASIC for field equations, the same properties 
that make it so efficient for training large AI models (ie: data flow based matrix multiplications)
end up making it efficient for solving field equations with large numbers of voxels or cells.

Recent Achievement
==================
this is a test of youtube

.. raw:: html
   <iframe width="560" height="315" src="https://www.youtube.com/embed/7NWe0kwaqrE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

this is the end of the test

License
=======

This work is a product of the Federal Government.  As of now, it is covered
under a `CC0 License <https://creativecommons.org/share-your-work/public-domain/cc0/>`_ until
applications for a different copyright and license structure are adopted.

Setup Instructions
==================

There are two tasks to complete in order to get set up with the WFA.  First, the WFA library must be obtained.
Due to export restrictions, the code is maintained on a private gitlab server.  Follow the instructions in the
"Obtaining the Code" section.  Second, the compilers necessary to run the code are proprietary to Cerebras Systems, Inc.
The compilers are usually packaged with the hardware in singularity containers.  The compilers necessary for the WFA
have been integrated into version 1.8.0 and above in the SDK container.

Obtaining the Code
------------------
This github repository contains documentation only.  The full code repository is available `here <https://mfix.netl.doe.gov/gitlab/tjordan/cerebrasdev/-/tree/master/WSE_Field_Equation>`_.
To obtain access to the code, register on the `MFiX website <https://mfix.netl.doe.gov/register>`_
contact Terry Jordan at terry.jordan@netl.doe.gov to be added to the privet
gitlab repository.  This process has been adopted until NETL makes a determination about the export control nature of
this work.

After being added to the repository, checkout the latest repo:

.. code-block:: console

   git clone https://mfix.netl.doe.gov/gitlab/tjordan/cerebrasdev.git

Neocortex Instructions
----------------------
`Necortex <http://https://www.cmu.edu/psc/aibd/neocortex/>`_ is an NSF funded computer run by the Pittsburgh Supercomputing Center (PSC) and consists of a
Superdome Flex and two CS-2's (the system that contains a WSE). PSC runs regular user solicitations
for time on Neocortex.  NETL has been working with Cerebras and PSC to ensure the WFA can run on the
Neocortex system.

After obtaining access to the WFA and cloning on the Neocortex. Create a directory to store the python site packages:

.. code-block:: console

    mkdir <path_to>/Cerebras-extra-python-packages

Generate the hardware run using singularity (Note: PSC has not yet updated their system to 1.8.0, so a custom container
is needed.  Contact Leighton Wilson on the Neocortex slack channel to get access):

.. code-block:: console

    singularity run -B <path_to>/Cerebras-extra-python-packages:$HOME/.local -B <path_to_repo>/:/cerebras_dev <sif_location>/cbcore-31399de2fa.sif

Deactivate python (this will be needed if you are using conda to manage python):

.. code-block:: console

    conda deactivate

Add portrait to path:

.. code-block:: console

    export PATH=/cbcore/technology/sysroot-x86/bin/portrait:$PATH

Generate the hardware run:

.. code-block:: console

    cd <run_folder_name>
    python <path_to_repo>/build_and_test.py -st 1 && python <run_name>.py -hin <hardware_run_name> -mg bench.img <additional_args>

Copy hardware run to util directory:

.. code-block:: console

    cp <hardware_run_name>.tar.gz <path_to_repo>/WSE_Field_Equation/util

Submit job to hardware:

.. code-block:: console

    sbatch --nodelist=sdf-<1 or 2> neocortex_slurm_script_bash -c <hardware_run_name> -o <field_variable>

Check out SDF node:

.. code-block:: console

    srun --nodelist=sdf-<1 or 2> --pty bash

The results are currently stored on the drives local the specified sdf node:

.. code-block:: console

    cd /local1/<project_name>/<hardware_run_name>


Simple Diffusion Example
========================
This package is designed to allow practitioners to easily program the Wafer Scale Engine to
solve Field Equations at an exceptionally rapid pace with high energy efficiency.  This equation
can be solved both explicitly and implicitly with linear discretization to second order accuracy. The
WFA package is capable of solving both explicit and implicit formulations.

The governing equation for diffusion without convection takes a simple Laplacian form and is one of the
most common field equations that relates the time rate of change of a scalar field to its spatial distribution,
known as Fick's second law.  Here, :math:`\alpha` is the conductivity.

.. math::
    
    \frac{\partial T}{\partial t} =  \alpha \nabla^2 T = \alpha \left(\frac{\partial^2 T}{\partial x^2} + \frac{\partial^2 T}{\partial y^2} + \frac{\partial^2 T}{\partial z^2}\right)

Solving Explicitly
------------------

With explicit linear discretization on a cartesian, uniform grid, the diffusion equation simplifies to:

.. math::
    
    T_i^{n+1} = c\left(T_E^n + T_W^n + T_N^n + T_S^n + T_T^n + T_B^n \right) + \left( 1 - 6c\right) T_i^n
    
.. math::

    c = \frac{\alpha \left(\Delta t \right)}{\left(\Delta l \right)^2}
    
The equations will remain stable when:

.. math::

    3c < 0.5
    
From the stability criteria, it is clear that the allowable time step is proportional
to the square of the voxel length scale and inversely proportional to the conductivity.
Explicit solutions are very useful for large voxel size problems and/or those with
low conductivity.  Common applications for explicit solutions are weather and reservoir
modeling where voxel sizes are large and conductivity is low.

The following code shows how to solve a generic diffusion equation explicitly in the
WFA.

file name: rectangular_diffusion_explicit.py

.. code-block:: python

    from WSE_FE.WSE_Interface import WSE_Interface
    from WSE_FE.WSE_Array import WSE_Array
    from WSE_FE.WSE_Loops import WSE_For_loop
    import NumPy as np
    
    wse = WSE_Interface()
    parser = wse.get_cmd_line_parser()
    
    # Define constansts
    c = 0.1
    center = 1.0 - 6.0 * c
    
    # Add CFD specific command line arguments
    parser.add_argument("-x","--x", help="specify x dimension", type=int, default=10)
    parser.add_argument("-y","--y", help="specify y dimension", type=int, default=10)
    parser.add_argument("-z", "--z", help="specify z dimension", type=int, default=10)
    parser.add_argument("-ts","--time_steps", help="specify number of time steps", type=int, default=5)
    args = wse.get_available_cmd_args()
    
        
    # Create the initial Temperature field
    T_init = np.ones((args.x,args.y,args.z))

    #Set Boundary Conditions    
    T_init[1:-1,0,1:-1] = 1.0
    T_init[1:-1,-1,1:-1] = 1.0
    T_init[0,1:-1,1:-1] = 1.0
    T_init[-1,1:-1,1:-1] = 1.0
    T_init[1:-1,1:-1,0] = 0.0
    T_init[1:-1,1:-1,-1] = 0.0
    
    # Instantiate the WSE Array objects needed
    T_n = WSE_Array(name='T_n', initData=T_init)
    
    # Start looping over time
    with WSE_For_loop('time_loop', args.time_steps):
        # Update T from current values
        T_n[1:-1,0,0] = center * T_n[1:-1,0,0]\
                        + c*(T_n[2:,0,0] + T_n[:-2,0,0] \
                           + T_n[1:-1,1,0] + T_n[1:-1,0,-1] \
                           + T_n[1:-1,-1,0] + T_n[1:-1,0,1])
        
    # Compile, run, and validate
    wse.make_WSE(answer=T_n)
    
From the command line run:

.. code-block:: console
    
    $ python rectangular_diffusion_explicit.py
    
And it will produce an output similar to:

.. code-block:: console

    ********************* Starting Make ***********************
    ********************* Writing binary input files ***********************
        Total vectors to write:  3
        Total Data to Write (MB):  0.012
        Total T_bin to Write (MB):  0.00768
    **************** Done writing binary input files ***********************
    ********************* Started Host Validation ***********************
    ********************* Done Host Validation ***********************
    ********************* Running Cerebras Tools ***********************
    examples                :  0:01.42 PASS smoke [ H=4 W=4 D=100 NC=8 NArgs=6 COMPILER=angler ] 
    ********************* Finished Running Cerebras Tools ***********************
    ********************* Done Make ***********************

    
Solving Implicitly
------------------
Implicit methods can allow for larger time steps.  The fully implicit formulation with linear discretization
and a uniform grid simplified to:

.. math::

    T_i^{n+1} - \frac{c}{1+6c} \left(T_E^{n+1} + T_W^{n+1} + T_N^{n+1} + T_S^{n+1} + T_T^{n+1} + T_B^{n+1} \right) = \frac{1.0}{\left( 1 - 6c\right)} T_i^n

The convergence criteria for an implicit method is diagonal dominance.  In this case: :math:`1 \geq \frac{6c}{1+6c}`
which is always met.  This equation will converge, even if slowly, regardless of the conductivity or cell length scale.

The following code shows how to solve a generic diffusion equation implicitly in the
WFA.

file name: rectangular_diffusion.py

.. code-block:: python

    from WSE_FE.WSE_Interface import WSE_Interface
    from WSE_FE.WSE_Array import WSE_Array
    from WSE_FE.WSE_Loops import WSE_For_loop

    from WSE_FE.WSE_Solver import CgConstantOffDiag, JacobiConstantOffDiag
    import NumPy as np

    wse = WSE_Interface()
    parser = wse.get_cmd_line_parser()

    c = 0.1
    off_diag_coeff = -c/(1.0+6.0*c)
    source_center_coeff = 1.0/(1.0+6.0*c)

    #Add CFD Specific command line arguments
    parser.add_argument("-x","--x", help="specify x dimension", type=int, default=10)
    parser.add_argument("-y","--y", help="specify y dimension", type=int, default=10)
    parser.add_argument("-z", "--z", help="specify z dimension", type=int, default=10)
    parser.add_argument("-ts","--time_steps", help="specify number of time steps", type=int, default=5)
    parser.add_argument("-s","--solver", help="the solver to use (Cg or Jacobi with constant off diagonal)", type=str, default='Cg')
    args = wse.get_available_cmd_args()

    if args.solver == 'Cg':
        solver = CgConstantOffDiag(args.x, args.y, args.z)
    elif args.solver == 'Jacobi':
        solver = JacobiConstantOffDiag(args.x, args.y, args.z)

    #Create the initial Temperature field
    T_init = np.ones((args.x,args.y,args.z))

    T_init[1:-1,0,1:-1] = 1.0
    T_init[1:-1,-1,1:-1] = 1.0
    T_init[0,1:-1,1:-1] = 1.0
    T_init[-1,1:-1,1:-1] = 1.0
    T_init[1:-1,1:-1,0] = 0.0
    T_init[1:-1,1:-1,-1] = 0.0

    #Instantiate the WSE Array objects needed
    T_n = WSE_Array(name='T_n', initData=T_init)

    if args.solver == 'Cg':
        b = wse.get_named_array('program_scratch_2')
    else:
        b = WSE_Array(name='b', initData=np.zeros((args.x,args.y,args.z)))

    
    with WSE_For_loop('time_loop', args.time_steps):
        # calculate the source terms
        b[1:-1,0,0] = source_center_coeff*T_n[1:-1,0,0] 
    
        # run the generalized CG solver
        T_n = solver.solve(T_n, off_diag_coeff, b, 0.001)
        
    wse.make_WSE(answer=T_n)


From the command line run:

.. code-block:: console
    
    $ python rectangular_diffusion.py
    
And it will produce an output similar to:

.. code-block:: console

    ********************* Starting Make ***********************
    ********************* Writing binary input files ***********************
        Total vectors to write:  6
        Total Data to Write (MB):  0.024
        Total T_bin to Write (MB):  0.01536
    **************** Done writing binary input files ***********************
    ********************* Started Host Validation ***********************
    ********************* Done Host Validation ***********************
    ********************* Running Cerebras Tools ***********************
    examples                :  0:03.71 PASS smoke [ H=4 W=4 D=100 NC=8 NArgs=6 COMPILER=angler ] 
    ********************* Finished Running Cerebras Tools ***********************
    ********************* Done Make ***********************


Command Line Arguments
======================

The WFA uses the argparse library to automatically add compiler options
to any script that calls the WSE_FE.WSE_Interface.WSE_Interface.make_WSE method.
This feature enables rapid development, supports debugging, and compiling larges
scale executables for hardware runs with just a few keystrokes.

Just add `-h` behind any python script with the make_WSE method called in it to
get a help menu.  It is possible to add domain specific command line arguments
to a program with the `get_cmd_line_parser` and `get_available_cmd_args` methods
as seen in the above examples.  Use the WSE_Interface.str_to_bool method in as the input to 
the type keyword argument in the parser.add_argument method for boolean inputs.

The `-h` option will produce an output similar to:


.. code-block:: console

    usage: test_center_add.py [-h] [-dl DEBUG_LEVEL] [-lp LAUNCH_PORTRAIT]
                  [-mg MAKE_GOAL] [-vr VERIFY_RESULT]
                  [-ptv PRINT_TILE_VERIFICATION]
                  [-hin HARDWARE_IMAGE_NAME]
                  [-ast ANALYZE_SIMULATOR_TIMESTAMPS]
                  [-hon HARDWARE_OUTPUT_NAME] [-to TIMEOUT]
                  [-att ADD_TASK_TAGS] [-ts32 TIME_STAMPS_32BIT]
                  [-ts16 TIME_STAMPS_16BIT] [-tsm TIME_STAMP_MODULO]
                  [-ppc POWER_PERFORMANCE_COUNTER]
                  [-dml DISPLAY_MAKE_LOG] [-x X] [-y Y] [-z Z]

    Command Line Parser for WSE Interface
    
    optional arguments:
      -h, --help            show this help message and exit
      -dl DEBUG_LEVEL, --debug_level DEBUG_LEVEL
                            Sets the level of debug. 0:off 1:create portrait files
                            2:JOB=-
      -lp LAUNCH_PORTRAIT, --launch_portrait LAUNCH_PORTRAIT
                            launches portrait
      -mg MAKE_GOAL, --make_goal MAKE_GOAL
                            specify make goal
      -vr VERIFY_RESULT, --verify_result VERIFY_RESULT
                            toggle verification of results through NumPy
      -ptv PRINT_TILE_VERIFICATION, --print_tile_verification PRINT_TILE_VERIFICATION
                            enables printing the memory space from the host during
                            validation. Specify Tx.y where x and y are the tile
                            coordinates from passing
      -hin HARDWARE_IMAGE_NAME, --hardware_image_name HARDWARE_IMAGE_NAME
                            specifying a name creates files for a hardware run
      -ast ANALYZE_SIMULATOR_TIMESTAMPS, --analyze_simulator_timestamps ANALYZE_SIMULATOR_TIMESTAMPS
                            turns on analysis of time stamps from Simfabric
      -hon HARDWARE_OUTPUT_NAME, --hardware_output_name HARDWARE_OUTPUT_NAME
                            specifying on output image name turns on hardware
                            analysis
      -to TIMEOUT, --timeout TIMEOUT
                            specifies the allowable time for a Simfabric run
      -att ADD_TASK_TAGS, --add_task_tags ADD_TASK_TAGS
                            compiles task tags into ag files for debugging
      -ts32 TIME_STAMPS_32BIT, --time_stamps_32bit TIME_STAMPS_32BIT
                            set time stamps to 32 bit recording
      -ts16 TIME_STAMPS_16BIT, --time_stamps_16bit TIME_STAMPS_16BIT
                            set time stamps to 16 bit recording
      -tsm TIME_STAMP_MODULO, --time_stamp_modulo TIME_STAMP_MODULO
                            specify the frequency of recording time stamps
      -ppc POWER_PERFORMANCE_COUNTER, --power_performance_counter POWER_PERFORMANCE_COUNTER
                            turns on power performance counter recording
      -dml DISPLAY_MAKE_LOG, --display_make_log DISPLAY_MAKE_LOG
                            displays make log from wse make
      -x X, --x X           specify x dimension
      -y Y, --y Y           specify y dimension
      -z Z, --z Z           specify z dimension

Running Scripts for Development
------------------------------
Running the script without any command line options will cause the API to create the 
*.ag, *.paint, and Makefile, compile the code into a binary, run the 
hardware simulator (Simfabric) and compare the output to a NumPy based
validation code within the API.  The correct result is specified in
make_wse through the `answer` keyword argument. 

This configuration is a great way to debug the code during development.
It is not recommended that the problem size be very large if run in this mode.
Simfabric is very useful, but it is not very fast and can consume considerable
memory if the problem size is set to a large size.

If the code ran successfully and produced the same answer as the validation code it will produce an output
similar to:
    
.. code-block:: console

    ********************* Starting Make ***********************
    ********************* Writing binary input files ***********************
        Total vectors to write:  6
        Total Data to Write (MB):  0.024
        Total T_bin to Write (MB):  0.01536
    **************** Done writing binary input files ***********************
    ********************* Started Host Validation ***********************
    ********************* Done Host Validation ***********************
    ********************* Running Cerebras Tools ***********************
    examples                :  0:03.71 PASS smoke [ H=4 W=4 D=100 NC=8 NArgs=6 COMPILER=angler ] 
    ********************* Finished Running Cerebras Tools ***********************
    ********************* Done Make ***********************
    
The PASS indicates that Simfabric ran and the validation/Simfabric output provided in `answer` were identical.

Debugging
---------

The following command line set is quite useful.  The `-ptv T4.5` option causes the validation
code to print out the source and destination memory space for the tile at coordinates of  (4,5)
in validation.txt for every operation.  This can then be compared to the Cerebras
debug tool, Portrait, for each operation to debug the code.  The -dl 1 option launches
the portrait sever which can be connected to in a browser at `http://localhost:8080/paint.html`

.. code-block:: console

    $ python test_add.py -ptv T4.5 -dl 1
    
Setting debug_level to 2 is equivalent to setting `GOAL=debug JOB=-` flags in the Cerebras make process.
This will cause Simfabric to dump out detailed log files and launch the Portrait server.  It will
also ignore any tests and force a pass to allow portrait to launch even if a fatal condition was
detected in the Simfabric run.  This is useful when debugging failing microcode.

Setting debug_level to 1 or 2 will also turn on more printing to screen for the API

Compiling for Hardware
----------------------
It is not recommended to run the validation or Simfabric for large scale runs.  Running either the NumPy
validation and/or Simfabric at the maximum scales that the WSE can run can take many days to
complete on a single node.  It is thus recommended that development be done on the smallest size problem
that is still meaningful and that it then be scaled up for a hardware run and all validation be turned off.

This can be easily done through the command line as follows

.. code-block:: console

    $ python rectangular_diffusion.py -x 800 -y 1000 -z 2000 -vr 0 -mg bench.img -hin full_scale_implicit_diffusion
    
If the code contains any timers, they can be turned on with the `-ts16` or `-ts32` arguments

WSE Interface Module
====================
The WSE Inference Module is a singleton class which is instantiated in every
other class within the WSE_Package.  It was created this way to provide flexibility
to users and to maintain key program sequences and memory wherever it is used.
The WSE provides several key features to the API:

1) Maintains program sequences
2) Handles memory spaces with no need for user input
3) Handles controller specific operations as well as global operations (Looping, conditionals, global reductions, controller arithmetic)
4) Handles code optimization
5) Enables compiler directives
6) Compiles into WSE images and maps
7) Handles validation in NumPy

Almost all of this functionality is handled automatically and is not a necessary
concern for end users.

Classes and Methods
-------------------

.. automodule:: WSE_FE.WSE_Interface
    :members:

WSE Array Module
================
The WSE_Array class is designed to allow users to work with 3D arrays in a
similar way to NumPy.  In fact, user defined WSE_Arrays are initialized from
3D NumPy arrays through the `initData` keyword on instantiation.  The first two
axes in the NumPy arrays map to tile coordinates and axis 2 maps to the memory
axis on each tile.

Each WSE_Array is designed to represent field data within a cell/voxel in a 3D
structured finite element/volume simulation. Thus, the shape of each WSE_Array
is set to the number of cells in the cardinal axes (including boundary cells).

Each tile holds a column of cells in the third axis of the NumPy data.  Arithmetic
is written from the perspective of a single tile in the array.

Slicing a WSE_Array object is similar to NumPy but is in relative coordinates.
The first axis is usually a slice range which specifies the elements of each
local vector on each tile.  It is equivalent to the slicing in the third axis
of a 3d NumPy array.  The second two axes specify the tile position relative
to the local tile coordinate.  The second axis specifies the relative position
east and west, and the third axis specifies the relative position north and
south.

Arithmetic operations are done in the worker field.  When neighbor operations
are involved, moats send their data into the worker field.  Moats are designed
to hold NSEW boundary conditions and are usually only updated at the tail end
of a time step.  Boundary conditions can be updated through a copy operation
which will move neighboring values from the worker field into the moat.


WSE_Array format:

.. code-block:: python

    dst[1:-1, 0, 0] = s0[2:,0,0] + s1[:-2,0,0]
    dst[1:-1, 0, 0] = s0[1:-1,1,0] + s1[1:-1,0,0]

NumPy format:

.. code-block:: python

    dst[1:-1,1:-1,1:-1] = s0[1:-1,1:-1,2:] + s1[1:-1,1:-1,:-2]
    dst[1:-1,1:-1,1:-1] = s0[1:-1,2:,1:-1] + s1[1:-1,1:-1,1:-1]

Classes and Methods
-------------------

.. autoclass:: WSE_FE.WSE_Array.WSE_Array
    :members:
    :special-members: __add__, __sub__, __mul__, __truediv__, __getitem__, __setitem__
    
WSE Scalar Module
=================
The WSE Scalar Module is designed to handle global, mutable scalars in the
WFA.  Global scalars live in the memory space of the controller and
are broadcast with the arguments when an operation is evoked.  This approach
conserves memory on the system and incurs minimal latencies.  It is most
commonly used in calculating dot products in linear solvers.  The Module
supports a limited but growing number of scalar-scalar operations.  These 
operations are conducted on the controller tile.

Classes and Methods
-------------------

.. autoclass:: WSE_FE.WSE_Scalar.WSE_Scalar
    :members:
    :special-members: __add__, __sub__, __mul__, __truediv__, __getitem__, __setitem__    

WSE Solver Module
=================
This is a convenience class that packages common linear solvers to solve systems
of linear equations to a defined tolerance.  The Solvers use the same Modules
in this package to program the WSE once the coefficient matrix and source vector
are formed.  The implicit solution example for the diffusion equation
uses one of the `CgConstantOffDiag` variant below.  Be sure to instantiate
the class at the start of a program to initialize the necessary memory spaces.

The solvers will create `program_scratch` space for arrays and scalars.  These
memory spaces are available to users outside of the solver, but will be overwritten
when the solver is called.  This is usually ok as implicitly solving equations
involves repeated processes of forming and solving and the memory spaces during
each process is partially isolated (they share A/b and field values).  Reusing
program scratch is suitable for several intermediate calculations in the formation
steps.

Classes and Methods
-------------------

.. autoclass:: WSE_FE.WSE_Solver.BiCgStab
    :members:

.. autoclass:: WSE_FE.WSE_Solver.Cg
    :members:
    
.. autoclass:: WSE_FE.WSE_Solver.CgConstantOffDiag
    :members:

.. autoclass:: WSE_FE.WSE_Solver.Jacobi
    :members:
    
.. autoclass:: WSE_FE.WSE_Solver.JacobiConstantOffDiag
    :members:

WSE Loops Module
=================
The WSE Loops Module is designed to handle...

Classes and Methods
-------------------
    
.. autoclass:: WSE_FE.WSE_Loops.WSE_For_Loop
    :members:
    :special-members: __init__
    
.. autoclass:: WSE_FE.WSE_Loops.WSE_While_Loop
    :members:
    :special-members: __init__
    
.. autoclass:: WSE_FE.WSE_Loops.WSE_Solver_While_Loop
    :members:
    :special-members: __init__
    
CFD Formation Module
=================
The CFD Formation Module is designed to handle...

Classes and Methods
-------------------

.. autoclass:: WSE_FE.CFD_Formation._Formation_Base
    :members:
    
.. autoclass:: WSE_FE.CFD_Formation._Sliceable_Constant
    :members:
    :special-members: __init__
    
.. autoclass:: WSE_FE.CFD_Formation._FD_Nonlinear_Diffusion
    :members: 
    :special-members: __init__    
    
.. autoclass:: WSE_FE.CFD_Formation.Scalar_Transport
    :members:  
    
.. autoclass:: WSE_FE.CFD_Formation.Momentum_Colocated
    :members:  
    :special-members: __init__   
    
.. autoclass:: WSE_FE.CFD_Formation.Press_Colocated
    :members:  
    :special-members: __init__   
    
FD Navier Stokes Module
=================
The FD Navier Stokes Module is designed to handle...

Classes and Methods
-------------------

.. autoclass:: WSE_FE.FD_Navier_Stokes.FD_Stokes
    :members:
    :special-members: __init__  

Custom IDE With Synax Highlighting
==================================
NETL is maintaining a custom version of the Spyder IDE which supports syntax
highlighing for WSE code.  Installation can be done with conda by following the
instructions below:

1. clone the repository from the `gitlab <https://mfix.netl.doe.gov/gitlab/tjordan/spyder/-/tree/wse_support>`_ repository
2. check out the wse_support branch
3. navigate to the folder with the setup.py and env.yml files
4. run `conda env create -f env.yml --name spyder`
5. run `conda activate spyder`
6. run `python setup.py install`
7. run `spyder`

Coding Style Guidelines
=======================

Python
------
NETL strictly adheres to the PEP8 coding standard with a modified maximum line length of 120 characters.  Code will not
be accepted unless it meets these standards.  This is not onerous especially since modern IDEs like Spyder have code
style warning/guide systems built in.  It is suggested that users install the custom Spyder version above and make
use of the "Show code style warnings" option in the Source menu.  To adjust the line length go to Tools, preferences,
Completion and linting, Code style and formatting, and then set the "Maximum allowed line length" to 120.

WSE files
---------
no coding standard has been adopted yet, but will be coming.

Python Docstrings
-----------------
Please follow the Numpy docstring format.  In spyder, a template can be autogenerated using Ctrl+Alt+D right after
the method/class definition.

